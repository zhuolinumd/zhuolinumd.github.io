<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Recognizing Actions by Shape-Motion Prototype Trees</title>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <style>
    :root{
      --bg:#eef4f8;
      --card:#ffffff;
      --text:#0c1222;
      --muted:rgba(12,18,34,.72);
      --line:rgba(12,18,34,.12);
      --accent:#2f6bff;
      --shadow:0 12px 30px rgba(12,18,34,.10);
      --radius:16px;
      --max:1040px;
    }

    *{ box-sizing:border-box; }
    body{
      margin:0;
      font-family:Inter, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
      background:var(--bg);
      color:var(--text);
      line-height:1.65;
    }

    a{ color:var(--accent); text-decoration:none; }
    a:hover{ text-decoration:underline; text-underline-offset:3px; }

    .wrap{
      max-width:var(--max);
      margin:0 auto;
      padding:26px 18px 60px;
    }

    .card{
      margin-top:18px;
      padding:18px;
      background:var(--card);
      border:1px solid var(--line);
      border-radius:var(--radius);
      box-shadow:var(--shadow);
    }

    h1{
      font-size:30px;
      letter-spacing:-0.5px;
      margin:0 0 6px;
      text-align:center;
    }

    .authors{
      text-align:center;
      font-size:18px;
      margin-bottom:10px;
    }
    .authors a{ font-weight:700; }

    h2{
      font-size:20px;
      margin-top:0;
      border-bottom:1px solid rgba(12,18,34,.10);
      padding-bottom:6px;
    }

    p{
      font-size:16px;
      text-align:left;
    }

    ul{
      margin-left:20px;
    }
    li{
      margin:8px 0;
      font-size:16px;
    }

    .figure{
      text-align:center;
      margin:12px 0;
    }
    .figure img{
      max-width:100%;
      height:auto;
      border-radius:12px;
      border:1px solid var(--line);
      background:#fff;
    }

    .video{
      text-align:center;
      margin:12px 0;
    }

    .footer{
      margin-top:18px;
      font-size:13px;
      color:var(--muted);
      text-align:center;
      font-weight:700;
    }
  </style>
</head>

<body>
  <div class="wrap">
    <section class="card">
      <h1>Recognizing Human Actions by Learning and Matching Shape-Motion Prototype Trees</h1>
      <div class="authors">
        <a href="https://zhuolinumd.github.io/">Zhuolin Jiang</a>,
        <a href="http://www.adobe.com/technology/people/sanjose/lin.html">Zhe Lin</a>,
        <a href="http://www.umiacs.umd.edu/~lsd/">Larry S. Davis</a>
      </div>
    </section>

    <section class="card">
      <h2>Abstract</h2>
      <p>
        A shape-motion prototype-based approach is introduced for action recognition. The approach represents an action
        as a sequence of prototypes for efficient and flexible action matching in long video sequences. During training,
        an action prototype tree is learned in a joint shape and motion space via hierarchical K-means clustering and each
        training sequence is represented as a labeled prototype sequence; then a look-up table of prototype-to-prototype
        distances is generated. During testing, based on a joint probability model of the actor location and action
        prototype, the actor is tracked while a frame-to-prototype correspondence is established by maximizing the joint
        probability, which is efficiently performed by searching the learned prototype tree; then actions are recognized
        using dynamic prototype sequence matching. Distance measures used for sequence matching are rapidly obtained by
        look-up table indexing, which is an order of magnitude faster than brute-force computation of frame-to-frame
        distances. Our approach enables robust action matching in challenging situations (such as moving cameras, dynamic
        backgrounds) and allows automatic alignment of action sequences. Experimental results demonstrate that our
        approach achieves recognition rates of 92.86 percent on a large gesture data set (with dynamic backgrounds),
        100 percent on the Weizmann action data set, 95.77 percent on the KTH action data set, 88 percent on the UCF
        sports data set, and 87.27 percent on the CMU action data set.
      </p>
    </section>

    <section class="card">
      <h2>Feature Extraction</h2>
      <p>
        Examples of extracting the shape-motion descriptor from a test frame.
        (a) Input silhouette; (b) image gradient field; (c) raw optical flow field;
        (d) compensated optical flow field; (e) silhouette-based shape descriptor;
        (f) HOG-based shape descriptor; (g) motion descriptor computed from the raw
        optical flow field; (h) motion descriptor computed from the compensated optical
        flow field.
      </p>
      <div class="figure">
        <img src="featuredescriptor.png" width="700" height="400" />
      </div>
    </section>

    <section class="card">
      <h2>Keck Gesture Dataset</h2>
      <p>
        The gesture dataset consisting of 14 different gesture classes, which are a subset of military signals.
        The following figure shows sample training frames of this dataset. The gesture dataset is collected using
        a color camera with 640 × 480 resolution. Each of the 14 gestures is performed by three people. In each
        sequence, the same gesture is repeated three times by each person. Hence there are 3 × 3 × 14 = 126
        video sequences for training which are captured using a fixed camera with the person viewed against a
        simple, static background. There are 4 × 3 × 14 = 168 video sequences for testing which are captured
        from a moving camera and in the presence of background clutter and other moving objects.
      </p>
      <div class="figure">
        <img src="sample images.png" width="1034" height="309" />
      </div>
    </section>

    <section class="card">
      <h2>Demo Video</h2>
      <div class="video">
        <iframe width="425" height="344"
          src="https://www.youtube.com/embed/JMxLeJKpWrA"
          title="YouTube video player"
          frameborder="0"
          allowfullscreen>
        </iframe>
      </div>
    </section>

    <section class="card">
      <h2>Downloads</h2>
      <p><strong>*All the materials provided here are only available for noncommercial reserach use.</strong></p>
      <p>
        All sequences are stored using AVI file format (MPEG V3-compressed version is available on-line).
        Uncompressed version is available on demand. There are 42 training video files and 56 testing video
        files. Each file contains about three subsequences used as a sequence in our experiments.
      </p>

      <ul>
        <li><a href="PrototypeTree/sequences.txt">Sequences.txt</a></li>
        <li><a href="https://drive.google.com/file/d/19QL1MiLTVJuP1rkQE5vREv_Q0lLFhka6/view?usp=share_link">Keck Gesture Dataset</a></li>
        <li><a href="PrototypeTree/bgsubtraction.zip">Background Subtraction for Training Set</a></li>
        <li><a href="PrototypeTree/KTHBoundingBoxInfo.txt">Bounding Boxes file for KTH Dataset</a></li>
        <li><a href="PrototypeTree/shapemotiondescriptor.zip">Shape-motion Descriptor Extraction</a></li>
      </ul>

      <p>If you happen to use the dataset or other files provided by this webpage, please cite one of the following papers:</p>
      <ul>
        <li>
          Zhe Lin, Zhuolin Jiang, and Larry S. Davis,
          “Recognizing Actions by Shape-Motion Prototype Trees,”
          IEEE ICCV 2009.
          [<a href="Publications/ICCV2009_ActionSMPT.pdf">pdf</a>]
          [<a href="Publications/ICCV2009Slide.pdf">slide</a>]
        </li>
        <li>
          Zhuolin Jiang, Zhe Lin, and Larry S. Davis,
          “Recognizing Human Actions by Learning and Matching Shape-Motion Prototype Trees,”
          IEEE TPAMI 2012.
          [<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6136521">pdf</a>]
        </li>
      </ul>

      <p>If you have any inquires or questions about this dataset, please contact:</p>
      <p>
        Zhuolin Jiang (<a href="mailto:zhuolinumd@gmail.com">zhuolinumd@gmail.com</a>)
      </p>
    </section>

    <footer class="footer">
      Latest update 01-27-2026 · © 2026 Zhuolin Jiang
    </footer>

  </div>
</body>
</html>