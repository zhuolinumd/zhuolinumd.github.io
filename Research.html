<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Research | Zhuolin Jiang</title>
  <meta name="description" content="Research — Zhuolin Jiang" />

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <style>
    :root{
      --bg: #eef4f8;
      --card: #ffffff;
      --text: #0c1222;
      --muted: rgba(12,18,34,.72);
      --line: rgba(12,18,34,.12);
      --accent: #2f6bff;
      --shadow: 0 12px 30px rgba(12,18,34,.10);
      --radius: 16px;
      --max: 1040px;
    }

    *{ box-sizing:border-box; }
    html,body{ height:100%; }
    body{
      margin:0;
      font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
      color: var(--text);
      background: var(--bg);
      line-height: 1.65;
    }

    a{ color: var(--accent); text-decoration: none; }
    a:hover{ text-decoration: underline; text-underline-offset: 3px; }

    .wrap{ max-width: var(--max); margin: 0 auto; padding: 26px 18px 60px; }

    .topbar{
      display:flex;
      align-items:center;
      justify-content:space-between;
      gap:14px;
      padding: 10px 12px;
      border: 1px solid var(--line);
      border-radius: 999px;
      background: rgba(255,255,255,.5);
      backdrop-filter: blur(10px);
      box-shadow: var(--shadow);
    }
    .brand{
      display:flex;
      align-items:center;
      gap:10px;
      min-width: 220px;
    }
    .dot{
      width:10px; height:10px; border-radius:50%;
      background: linear-gradient(135deg, var(--accent), rgba(123,255,209,.8));
      box-shadow: 0 0 0 4px rgba(47,107,255,.14);
      flex: 0 0 auto;
    }
    .brand-title{ font-weight: 800; letter-spacing: .2px; }
    .brand-sub{ color: var(--muted); font-size: 13px; margin-top: -2px; }

    .nav{
      display:flex; flex-wrap:wrap; gap:10px;
      justify-content:flex-end;
    }
    .nav a{
      font-size:14px;
      font-weight:700;
      color: rgba(12,18,34,.72);
      padding: 8px 10px;
      border-radius: 12px;
      border: 1px solid transparent;
    }
    .nav a:hover{
      color: var(--text);
      border-color: var(--line);
      background: rgba(255,255,255,.80);
      text-decoration: none;
    }
    .nav a.active{
      color: var(--text);
      border-color: var(--line);
      background: rgba(255,255,255,.90);
      text-decoration: none;
    }

    .card{
      margin-top: 18px;
      border: 1px solid var(--line);
      background: rgba(255,255,255,0.4);
      border-radius: var(--radius);
      box-shadow: var(--shadow);
    }

    .pagehead{
      padding: 18px;
    }
    .pagehead h1{
      margin: 0 0 6px;
      font-size: 26px;
      letter-spacing: -0.4px;
      line-height: 1.2;
    }
    .pagehead p{
      margin: 0;
      color: var(--muted);
      font-size: 16px;
    }

    .toc{
      padding: 16px 18px 18px;
    }
    .toc h2{
      margin: 0 0 10px;
      font-size: 18px;
      letter-spacing: -.2px;
    }
    .toc ul{ margin: 8px 0 0 18px; }
    .toc li{ margin: 6px 0; }
    .toc a{ font-weight: 700; }

    .section{
      padding: 16px 18px 18px;
    }
    .section h2{
      margin: 0 0 10px;
      font-size: 20px;
      letter-spacing: -.2px;
    }
    .section p{
      margin: 10px 0 12px;
      color: rgba(12,18,34,.85);
      font-size: 16px;
    }
    .section ul{ margin: 8px 0 0 18px; }
    .section li{ margin: 8px 0; color: rgba(12,18,34,.85); font-size: 16px; }
    .section .paper-title{ font-weight: 800; color: var(--text); }
    .section .pdf-link{ font-weight: 800; }

    .figure{
      display:flex;
      justify-content:center;
      margin: 10px 0 12px;
    }
    .figure img{
      max-width: 100%;
      height: auto;
      border-radius: 14px;
      border: 1px solid var(--line);
      background: rgba(12,18,34,.02);
    }

    .footer{
      margin-top: 18px;
      color: rgba(12,18,34,.65);
      font-size: 13px;
      display:flex;
      justify-content: space-between;
      flex-wrap: wrap;
      gap: 10px;
      opacity: .95;
    }
  </style>
</head>

<body>
  <div class="wrap">
    <header class="topbar">
      <div class="brand">
        <span class="dot" aria-hidden="true"></span>
        <div>
          <div class="brand-title">Zhuolin Jiang</div>
          <div class="brand-sub">Principal Research Scientist · RTX</div>
        </div>
      </div>

      <nav class="nav" aria-label="Primary">
        <a href="index.html">Home</a>
        <a href="Publications.html">Publications</a>
        <a href="https://scholar.google.com/citations?user=X55lT7kAAAAJ&amp;hl=en" target="_blank" rel="noopener">Citations</a>
        <a class="active" href="Research.html">Research</a>
        <a href="Resources.html">Resources</a>
        <a href="Resume/resume_Dec2016.pdf">Resume</a>
      </nav>
    </header>

    <section class="card pagehead">
      <h1>Research</h1>
      <p>My research focuses on Computer Vision, Pattern Recognition and Machine Learning, specifically on the following topics:</p>
    </section>

    <section class="card toc">
      <h2>Topics</h2>
      <ul>
        <li><a href="#submodular">Submodular Optimization for Vision</a></li>
        <li><a href="#sparse">Sparse Coding and Dictionary Learning</a></li>
        <li><a href="#lowrank">Low-Rank Matrix Recovery for Vision</a></li>
        <li><a href="#clustering">Unsupervised and Supervised Clustering</a></li>
        <li><a href="#transfer">Transfer Learning</a></li>
      </ul>
    </section>

    <section class="card section" id="submodular">
      <h2>Submodular Optimization for Vision</h2>
      <p>
        Submodularity is an intuitive diminishing returns property, stating that adding an element to a smaller set helps
        more than adding it to a larger set. Submodularity allows one to efficiently find (near-)optimal solutions, which is
        useful in a lot of vision applications. My research aims to use submodularity optimization to solve various vision problems.
      </p>

      <div class="figure">
        <img src="submodularsaliency1.png" width="837" height="350" alt="Submodular Saliency" />
      </div>
      <ul>
        <li>
          Zhuolin Jiang, Larry S. Davis.
          &quot;<span class="paper-title">Submodular Salient Region Detection</span>&quot;.
          IEEE Conference on Computer Vision and Pattern Recognition, 2013.
          [<a class="pdf-link" href="Publications/CVPR2013_SubmodularSaliency.pdf">pdf</a>]
        </li>
      </ul>

      <div class="figure">
        <img src="submodularDL.png" width="734" height="200" alt="Submodular Dictionary Learning" />
      </div>
      <ul>
        <li>
          Zhuolin Jiang, Guangxiao Zhang, Larry S. Davis.
          &quot;<span class="paper-title">Submodular Dictionary Learning for Sparse Coding</span>&quot;.
          IEEE Conference on Computer Vision and Pattern Recognition, 2012.
          [<a class="pdf-link" href="Publications/SubmodularDL.pdf">pdf</a>]
        </li>
      </ul>

      <div class="figure">
        <img src="actionattribute.png" width="981" height="250" alt="Action Attributes" />
      </div>
      <ul>
        <li>
          Qiang Qiu, Zhuolin Jiang, Rama Chellappa.
          &quot;<span class="paper-title">Sparse Dictionary-based Representation and Recognition of Action Attributes</span>&quot;.
          IEEE Conference on Computer Vision, 2011.
          [<a class="pdf-link" href="Publications/DL_MMI_GP.pdf">pdf</a>]
        </li>
      </ul>
    </section>

    <section class="card section" id="sparse">
      <h2>Sparse Coding and Dictionary Learning</h2>
      <p>
        Sparse coding approximate an input signal as a linear combination of a few items from a predefined and learned dictionary.
        It usually achieves state-of-the-arts in all kinds of vision applications. The performance of sparse coding relies on the
        quality of dictionary. My research aims to learn a discriminative dictionary for recognition.
      </p>

      <div class="figure">
        <img src="lcksvd1.png" width="1002" height="200" alt="LC-KSVD" />
      </div>
      <ul>
        <li>
          Zhuolin Jiang, Zhe Lin, Larry S. Davis.
          &quot;<span class="paper-title">Learning a Discriminative Dictionary for Sparse Coding via Label Consistent K-SVD</span>&quot;.
          IEEE Conference on Computer Vision and Pattern Recognition, 2011.
          [<a class="pdf-link" href="Publications/0198.pdf">pdf</a>]
        </li>
        <li>
          Zhuolin Jiang, Zhe Lin, Larry S. Davis.
          <span class="paper-title">Label Consistent K-SVD: Learning A Discriminative Dictionary for Recognition</span>.
          IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013.
          [<a class="pdf-link" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6516503" target="_blank" rel="noopener">pdf</a>]
        </li>
      </ul>

      <div class="figure">
        <img src="DDL2.png" width="1117" height="360" alt="Discriminative Dictionary Learning with Pairwise Constraints" />
      </div>
      <ul>
        <li>
          Huimin Guo*, Zhuolin Jiang*, Larry S. Davis.
          &quot;<span class="paper-title">Discriminative Dictionary Learning with Pairwise Constraints</span>&quot;.
          Asian Conference on Computer Vision, 2012. (ORAL) (* indicates equal contribution, <span class="paper-title">Best Student Paper Award</span>).
          [<a class="pdf-link" href="Publications/DDLPairWise.pdf">pdf</a>]
        </li>
      </ul>

      <div class="figure">
        <img src="onlineSSDL.png" width="1057" height="220" alt="Online Semi-supervised Discriminative Dictionary Learning" />
      </div>
      <ul>
        <li>
          Guangxiao Zhang, Zhuolin Jiang, Larry S. Davis.
          &quot;<span class="paper-title">Online Semi-supervised Discriminative Dictionary Learning for Sparse Representation</span>&quot;.
          Asian Conference on Computer Vision, 2012.
          [<a class="pdf-link" href="Publications/OnlineDDL.pdf">pdf</a>]
        </li>
      </ul>

      <div class="figure">
        <img src="tagtaxonomy.png" width="663" height="230" alt="Tag Taxonomy Aware Dictionary Learning" />
      </div>
      <ul>
        <li>
          Jingjing Zheng, Zhuolin Jiang.
          &quot;<span class="paper-title">Tag Taxonomy Aware Dictionary Learning for Region Tagging</span>&quot;.
          IEEE Conference on Computer Vision and Pattern Recognition, 2013.
          [<a class="pdf-link" href="Publications/CVPR2013_RegionTagging.pdf">pdf</a>]
        </li>
      </ul>

      <div class="figure">
        <img src="TSC.png" width="663" height="300" alt="Discriminative Tensor Sparse Coding" />
      </div>
      <ul>
        <li>
          Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis.
          &quot;<span class="paper-title">Discriminative Tensor Sparse Coding for Image Classification</span>&quot;.
          British Machine Vision Conference, 2013.
          [<a class="pdf-link" href="Publications/bmvc2013_tensor_sparse.pdf">pdf</a>]
        </li>
      </ul>
    </section>

    <section class="card section" id="lowrank">
      <h2>Low-Rank Matrix Recovery for Vision</h2>
      <p>
        A common modeling assumption in many applications is that the underlying data lies (approximately) on a low-dimensional linear subspace.
        That is, a matrix X can be decomposed into two matrices: X = A+E, where A is a low-rank matrix and E is a sparse matrix.
        Low-rank matrix recovery which determines the low-rank matrix A from X, has been successfully applied to many applications.
        My research aims to use this technique for multi-class classification.
      </p>

      <div class="figure">
        <img src="lowrank.png" width="576" height="400" alt="Low-rank Matrix Recovery" />
      </div>
      <ul>
        <li>
          Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis.
          &quot;<span class="paper-title">Learning Structured Low-rank Representations for Image Classification</span>&quot;.
          IEEE Conference on Computer Vision and Pattern Recognition, 2013.
          [<a class="pdf-link" href="Publications/CVPR2013_LowRank.pdf">pdf</a>]
        </li>
      </ul>
    </section>

    <section class="card section" id="clustering">
      <h2>Unsupervised and Supervised Clustering</h2>
      <p>
        Data clustering is an important task in vision. I used it to learn action prototypes (or action prototype tree).
        A large number of studies aim to improve clustering by using supervision in the form of pairwise constraint or category information of each point.
        I used the category information to enforce discriminativeness for each cluster so the final clusters good for classification.
      </p>

      <div class="figure">
        <img src="actionprototype.png" width="665" height="400 alt="Action Prototype Trees" />
      </div>
      <ul>
        <li>
          Zhe Lin, Zhuolin Jiang, Larry S. Davis.
          &quot;<span class="paper-title">Recognizing Actions by Shape-Motion Prototype Trees</span>&quot;.
          IEEE Conference on Computer Vision, 2009.
          [<a class="pdf-link" href="Publications/ICCV2009_ActionSMPT.pdf">pdf</a>]
        </li>
        <li>
          Zhuolin Jiang, Zhe Lin, Larry S. Davis,
          &quot;<span class="paper-title">Recognizing Human Actions by Learning and Matching Shape-Motion Prototype Trees</span>&quot;.
          IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34(3): 533-547.
          [<a class="pdf-link" href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6136521" target="_blank" rel="noopener">pdf</a>]
        </li>
      </ul>

      <div class="figure">
        <img src="unifiedtree.png" width="877" height="300" alt="Unified Tree-based Framework" />
      </div>
      <ul>
        <li>
          Zhuolin Jiang, Zhe Lin, Larry S. Davis.
          &quot;<span class="paper-title">A Tree-based Approach to Integrated Action Localization, Recognition and Segmentation</span>&quot;.
          ECCV Workshop on Human Motion, 2010.
          [<a class="pdf-link" href="http://humanmotion.rutgers.edu/HM/W02.006.pdf" target="_blank" rel="noopener">pdf</a>]
        </li>
        <li>
          Zhuolin Jiang, Zhe Lin, Larry S. Davis.
          &quot;<span class="paper-title">A Unified Tree-based Framework for Joint Action Localization, Recognition and Segmentation</span>&quot;.
          Computer Vision and Image Understanding, 2012.
          [<a class="pdf-link" href="http://www.sciencedirect.com/science/article/pii/S1077314212001749" target="_blank" rel="noopener">pdf</a>]
        </li>
      </ul>

      <div class="figure">
        <img src="cckmcombined.png" width="889" height="270" alt="Class Consistent k-means" />
      </div>
      <ul>
        <li>
          Zhuolin Jiang, Zhe Lin, Larry S. Davis,
          &quot;<span class="paper-title">Class Consistent k-means: Application to Face and Action Recognition</span>&quot;.
          Computer Vision and Image Understanding, 2012, 116(6): 730-741.
          [<a class="pdf-link" href="http://www.sciencedirect.com/science/article/pii/S1077314212000367?v=s5" target="_blank" rel="noopener">pdf</a>]
        </li>
      </ul>
    </section>

    <section class="card section" id="transfer">
      <h2>Transfer Learning</h2>
      <p>
        Many learning approaches work well only under a common assumption: training and testing data are drawn from the same feature space and distribution.
        In many practical applications, the assumption may not hold. In such cases, transfer learning between task domains would be desirable since it is
        expensive to recollect training data and rebuild the model. My research aims to transfer knowledge across domains and transfer from multiple such source domains.
      </p>

      <div class="figure">
        <img src="TransferableDL.png" width="732" height="250" alt="Transferable Dictionary Pair" />
      </div>
      <ul>
        <li>
          Jingjing Zheng, Zhuolin Jiang, Jonathon Phillips, Rama Chellappa.
          &quot;<span class="paper-title">Cross-View Action Recognition via a Transferable Dictionary Pair</span>&quot;.
          British Machine Vision Conference, 2012. (ORAL).
          [<a class="pdf-link" href="Publications/TransferLearningviaTDP.pdf">pdf</a>]
        </li>
      </ul>

      <div class="figure">
        <img src="JointDL.png" width="500" height="270"  alt="View-invariant Sparse Representations" />
      </div>
      <ul>
        <li>
          Jingjing Zheng, Zhuolin Jiang.
          &quot;<span class="paper-title">Learning View-invariant Sparse Representations for Cross-view Action Recognition</span>&quot;.
          IEEE Conference on Computer Vision, 2013.
          [<a class="pdf-link" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zheng_Learning_View-Invariant_Sparse_2013_ICCV_paper.pdf"
              target="_blank" rel="noopener">pdf</a>]
        </li>
      </ul>
    </section>

    <footer class="footer">
      <div>© <span id="y"></span> Zhuolin Jiang</div>
      <div><a href="#top" onclick="window.scrollTo({top:0, behavior:'smooth'}); return false;">Back to top</a></div>
    </footer>

  </div>

  <script>
    document.getElementById("y").textContent = new Date().getFullYear();
  </script>
</body>
</html>
