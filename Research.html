<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<title>Research</title>
<style type="text/css">
body {
	background-image: url(bg23.gif);
	text-align: center;
	color: #000;
}
.text {font-weight:bold
}
.size4 {font-style: normal;
	text-align:left;
}
.size4 {text-align: left;
}
</style>
</head>

<body>
<table width="1120" height="6182" border="0" align="center">
  <tr>
    <td width="1114" align="left" bgcolor="#FFFFFF"><p align="center"; style="font-size:24px">[<a href="index.html"; style="color:#06F">Home</a>] [<a href="Publications.html"; style="color:#06F">Publications</a>] [<a href="http://scholar.google.com/citations?user=X55lT7kAAAAJ&amp;hl=en"; style="color:#06F">Citations</a>] [<a href="Resources.html"; style="color:#06F">Resources</a>]</p>
      <p style="font-size:22px">My Research focuses on Computer Vision, Pattern Recognition and Machine Learning, specifically on the following topics:</p>
      <ul>
      <li style="font-size:22px"; TYPE="SQUARE"><a href="#submodular"; style="color:#06F">Submodular Optimization for Vision</a></li>
      <li style="font-size:22px"; TYPE="SQUARE"><a href="#sparse"; style="color:#06F">Sparse Coding and Dictionary  Learning</a></li>
      <li style="font-size:22px"; TYPE="SQUARE"><a href="#lowrank"; style="color:#06F">Low-Rank Matrix Recovery for Vision</a></li>
      <li style="font-size:22px"; TYPE="SQUARE"><a href="#clustering"; style="color:#06F">Unsupervised and Supervised  Clustering</a></li>
      <li style="font-size:22px"; TYPE="SQUARE"><a href="#transfer"; style="color:#06F">Transfer Learning</a></li>
     
    </ul></td>
  </tr>
  <tr>
    <td bgcolor="#FFFFFF"><p style="margin:2px 0; font-size:22px; font-weight:bold; text-align:left; font-family: Tahoma, Geneva, sans-serif; text-shadow:2px 2px 2px #8F8F8F;"><a name="submodular">Submodular  Optimization for Vision</a></td>
  </tr>
  <tr>
    <td height="1199" align="left" bgcolor="#FFFFFF"><p style="font-size:21px; font-family:Arial, Helvetica, sans-serif">Submodularity is an intuitive diminishing returns property, stating that adding an element to a smaller set helps more than adding it to a larger set. Submodularity allows one to efficiently find (near-)optimal solutions, which is useful in a lot of vision applications. My research aims to use submodularity optimization to solve various vision problems.
    </p>
      <p align="center"><img src="submodularsaliency1.png" width="837" height="350" /></p>
      <ul>
        <li style="font-size:21px">Zhuolin Jiang, Larry S. Davis. &quot;<a class="text">Submodular Salient Region Detection</a>&quot;. IEEE  Conference on Computer Vision and Pattern Recognition, 2013. [<a href="Publications/CVPR2013_SubmodularSaliency.pdf"; style="color:#06F">pdf</a>]</li>
      </ul>
      <p>&nbsp;</p>
      <p align="center"><img src="submodularDL.png" width="734" height="200"/></p>
    <ul>
      <li style="font-size:21px">Zhuolin Jiang, Guangxiao Zhang, Larry S. Davis. &quot;<a class="text">Submodular Dictionary Learning for Sparse Coding</a>&quot;. IEEE Conference on  Computer Vision and Pattern Recognition, 2012. [<a href="Publications/SubmodularDL.pdf"; style="color:#06F">pdf</a>]</li>
      </ul>
    <p>&nbsp;</p>
    <p align="center"><img src="actionattribute.png" width="981" height="250" /></p>
    <ul>
      <li style="font-size:21px">Qiang Qiu, Zhuolin Jiang, Rama Chellappa. &quot;<a class="text">Sparse Dictionary-based Representation and Recognition of Action Attributes</a>&quot;. IEEE Conference on Computer Vision, 2011. [<a href="Publications/DL_MMI_GP.pdf"; style="color:#06F">pdf</a>]</li>
    </ul></td>
  </tr>
  <tr>
    <td bgcolor="#FFFFFF"><p style="margin:2px 0; font-size:22px; font-weight:bold; text-align:left; font-family: Tahoma, Geneva, sans-serif; text-shadow:2px 2px 2px #8F8F8F;"><a name="sparse">Sparse Coding and Dictionary  Learning</a></p></td>
  </tr>
  <tr>
    <td height="1588" align="left" bgcolor="#FFFFFF"><p style="font-size:21px; font-family:Arial, Helvetica, sans-serif">Sparse coding approximate an input signal as a linear combination of a few items from a predefined and learned dictionary. It usually achieves state-of-the-arts in all kinds of vision applications. The performance of sparse coding relies on the quality of dictionary. My research aims to learn a discriminative dictionary for recognition.
    </p>
      <p align="center"><img src="lcksvd1.png" width="1002" height="200" /></p>
    <ul>
      <li style="font-size:21px">Zhuolin Jiang, Zhe Lin, Larry S. Davis. &quot; <a class="text">Learning a Discriminative Dictionary for Sparse Coding via Label Consistent K-SVD</a>&quot;. IEEE  Conference on Computer Vision and Pattern Recognition, 2011</span>. [<a href="Publications/0198.pdf"; style="color:#06F">pdf</a>]</li>
      <li style="font-size:21px">Zhuolin Jiang, Zhe Lin, Larry S. Davis. <a class="text">Label Consistent K-SVD: Learning A Discriminative Dictionary for Recognition</a>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013. [<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6516503"; style="color:#06F">pdf</a>]</li>
      </ul>
    <p>&nbsp;</p>
    <p align="center"><img src="DDL2.png" width="1117" height="360" /></p>
    <ul>
      <li style="font-size:21px">Huimin Guo*, Zhuolin Jiang*, Larry S. Davis. &quot; <a class="text">Discriminative Dictionary Learning with Pairwise Constraints</a>&quot;. Asian Conference on Computer Vision, 2012. (ORAL) (* indicates equal contribution, <a class="text">Best Student Paper Award</a>). [<a href="Publications/DDLPairWise.pdf"; style="color:#06F">pdf</a>]</li>
      </ul>
    <p>&nbsp;</p>
    <p align="center"><img src="onlineSSDL.png" width="1057" height="220" /></p>
    <ul>
      <li style="font-size:21px">Guangxiao Zhang, Zhuolin Jiang, Larry S. Davis. &quot;<a class="text">Online Semi-supervised Discriminative Dictionary Learning for Sparse Representation</a>&quot;. Asian Conference on Computer Vision, 2012. [<a href="Publications/OnlineDDL.pdf"; style="color:#06F">pdf</a>]</li>
      </ul>
    <p>&nbsp;</p>
    <p align="center"><img src="tagtaxonomy.png" width="663" height="230" /></p>
    <ul>
      <li style="font-size:21px">Jingjing Zheng, Zhuolin Jiang. &quot;<a class="text">Tag Taxonomy Aware Dictionary Learning for Region Tagging</a>&quot;. IEEE Conference on Computer Vision and Pattern Recognition, 2013. [<a href="Publications/CVPR2013_RegionTagging.pdf"; style="color:#06F">pdf</a>]    </li>
    </ul>
    <p align="center"><img src="TSC.png" width="663" height="300"/></p>
    <ul>
      <li style="font-size:21px">Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis. &quot;<a class="text">Discriminative Tensor Sparse Coding for Image Classification</a>&quot;. British Machine Vision Conference, 2013. [<a href="Publications/bmvc2013_tensor_sparse.pdf"; style="color:#06F">pdf</a>]</li>
    </ul></td>
  </tr>
  
    <tr>
    <td align="left" bgcolor="#FFFFFF"><p style="margin:2px 0; font-size:22px; font-weight:bold; text-align:left; font-family: Tahoma, Geneva, sans-serif; text-shadow:2px 2px 2px #8F8F8F;"><a name="lowrank">Low-Rank Matrix Recovery for Vision</a></td>
  </tr>
  <tr>
    <td height="588" align="left" bgcolor="#FFFFFF"><p style="font-size:21px; font-family:Arial, Helvetica, sans-serif">A common modeling assumption in many applications is that the underlying data lies (approximately) on a low-dimensional linear subspace. That is, a matrix X can be decomposed into two matrices: X = A+E, where A is a low-rank matrix and E is a sparse matrix. Low-rank matrix recovery which determines the low-rank matrix A from X, has been successfully applied to many applications. My research aims to use this technique for multi-class classification.</p>
      <p align="center"><img src="lowrank.png" alt="" width="576" height="400" /></p>
      <ul>
        <li style="font-size:21px">Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis. &quot;<a class="text">Learning Structured Low-rank Representations for Image Classification</a>&quot;. IEEE  Conference on Computer Vision and Pattern Recognition, 2013. [<a href="Publications/CVPR2013_LowRank.pdf"; style="color:#06F">pdf</a>]</li>
    </ul></td>
  </tr>
  
  <tr>
    <td bgcolor="#FFFFFF"><p style="margin:2px 0; font-size:22px; font-weight:bold; text-align:left; font-family: Tahoma, Geneva, sans-serif; text-shadow:2px 2px 2px #8F8F8F;"><a name="clustering">Unsupervised and Supervised  Clustering</a></p></td>
  </tr>
  <tr>
    <td bgcolor="#FFFFFF" align="left"><p style="font-size:21px; font-family:Arial, Helvetica, sans-serif">Data clustering is an important task in vision. I used it to learn action prototypes (or action prototype tree). A large number of studies aim to improve clustering by using supervision in the form of pairwise constraint or category information of each point. I used the category information to enforce discriminativeness for each cluster so the final clusters good for classification.</p>
    <p align="center"><img src="actionprototype.png" width="665" height="400" /></p>
    <ul>
      <li><span style="font-size:21px">Zhe Lin, Zhuolin Jiang, Larry S. Davis. &quot;<a class="text">Recognizing Actions by Shape-Motion Prototype Trees</a>&quot;.  <span class="size4">IEEE Conference on Computer Vision, 2009. </span>[<a href="Publications/ICCV2009_ActionSMPT.pdf"; style="color:#06F">pdf</a>]</span></li>
      <li><span style="font-size:21px">Zhuolin Jiang,  Zhe Lin, Larry S. Davis, &quot;<a class="text">Recognizing Human Actions by Learning and Matching Shape-Motion Prototype Trees</a>&quot;. <span class="size4">IEEE Transactions on Pattern Analysis and  Machine Intelligence, 2012, 34(3): 533-547.</span> [<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6136521"; style="color:#06F">pdf</a>]</span></li>
      </ul>
    <p>&nbsp;</p>
    <p align="center"><img src="unifiedtree.png" width="877" height="300" /></p>
    <ul>
      <li style="font-size:21px">Zhuolin Jiang, Zhe Lin, Larry S. Davis. &quot;<a class="text">A Tree-based Approach to Integrated Action Localization, Recognition and Segmentation</a>&quot;. ECCV Workshop on Human Motion, 2010. [<a href="http://humanmotion.rutgers.edu/HM/W02.006.pdf"; style="color:#06F">pdf</a>]</li>
      <li style="font-size:21px">Zhuolin Jiang, Zhe Lin, Larry S. Davis. &quot;<a class="text">A Unified Tree-based Framework for Joint Action Localization, Recognition and Segmentation</a>&quot;. Computer Vision and Image Understanding, 2012. [<a href="http://www.sciencedirect.com/science/article/pii/S1077314212001749"; style="color:#06F">pdf</a>]</li>
      </ul>
    <p>&nbsp;</p>
    <p align="center"><img src="cckmcombined.png" width="889" height="270" /></p>
    <ul>
      <li style="font-size:21px">Zhuolin Jiang, Zhe Lin, Larry S. Davis, &quot;<a class="text">Class Consistent k-means: Application to Face and Action Recognition</a>&quot;. Computer Vision and Image Understanding, 2012, 116(6): 730-741. [<a href="http://www.sciencedirect.com/science/article/pii/S1077314212000367?v=s5"; style="color:#06F">pdf</a>]</li>
    </ul></td>
  </tr>
  <tr>
    <td bgcolor="#FFFFFF"><p style="margin:2px 0; font-size:22px; font-weight:bold; text-align:left; font-family: Tahoma, Geneva, sans-serif; text-shadow:2px 2px 2px #8F8F8F;"><a name="transfer">Transfer Learning</a></p></td>
  </tr>
  <tr>
    <td height="878" align="left" bgcolor="#FFFFFF"><p style="font-size:21px; font-family:Arial, Helvetica, sans-serif">Many learning approaches work well only under a common assumption: training and testing data are drawn from the same feature space and distribution. In many practical applications, the assumption may not hold. In such cases, transfer learning between task domains would be desirable since it is expensive to recollect training data and rebuild the model. My research aims to transfer knowledge across domains and transfer from multiple such source domains.</p>
    <p align="center"><img src="TransferableDL.png" width="732" height="250" /></p>
    <ul>
      <li style="font-size:21px">Jingjing Zheng, Zhuolin Jiang, Jonathon Phillips, Rama Chellappa. &quot;<a class="text">Cross-View Action Recognition via a Transferable Dictionary Pair</a>&quot;. British Machine Vision Conference, 2012. (ORAL). [<a href="Publications/TransferLearningviaTDP.pdf"; style="color:#06F">pdf</a>]</li>
    </ul>
    <p>&nbsp;</p>
    <p align="center"><img src="JointDL.png" width="500" height="270" /></p>
     <ul>
      <li style="font-size:21px">Jingjing Zheng, Zhuolin Jiang. &quot;<a class="text">Learning View-invariant Sparse Representations for Cross-view Action Recognition</a>&quot;. IEEE Conference on Computer Vision, 2013. [<a href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zheng_Learning_View-Invariant_Sparse_2013_ICCV_paper.pdf"; style="color:#06F">pdf</a>]</li>
     </ul>
    </td>
  </tr>
</table>
</body>
</html>
